data:
  sequence_iterator:
    batch_size: 16
    classname: CommonCorpusTokenSequenceLoader
    kwargs:
      sequence_length: 256
      shuffle_buffer_size: 16_384
  activations_harvester:
    llms:
      # - name: EleutherAI/pythia-160M
        # revision: step142000
      - name: EleutherAI/pythia-160M
        revision: step143000 # last checkpoint
    layer_indices_to_harvest: [8]
  activations_shuffle_buffer_size: 8_192
  cc_training_batch_size: 256
crosscoder:
  hidden_dim: 6144
train:
  optimizer:
    initial_learning_rate: 5e-5
    last_pct_of_steps: 0.2
  num_steps: 10_000
  save_every_n_steps: 1000
  log_every_n_steps: 20
  log_visualizations_every_n_steps: 1000
  upload_checkpoint_to_wandb_every_n_steps: 100

  c: 4.0
  lambda_s: 20.0
  lambda_p: 0.000003 # 3e-6
  jumprelu:
    backprop_through_jumprelu_threshold: False

experiment_name: l1_crosscoder_pythia_160M_layer_2
wandb: True